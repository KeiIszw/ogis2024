{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVKp29aoEYDF"
      },
      "source": [
        "# **WhisperSpeechSplitter**\n",
        "\n",
        "OpenAIのwhisperを用いて自動で文字起こしを行い、\n",
        "そのデータをもとに音源の分割を行います。音源はBGMが除去された声のみの素材が望ましいです。\n",
        "\n",
        "音声の切り出しを想定していますが、\n",
        "動画の自動切り出しも可能です(突貫工事)。音声より時間がかかりますが、使えなくはないかも。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<使用方法>\n",
        "\n",
        "各コードは、\"code\"と書かれた箇所の下の再生ボタンみたいなやつ![run_button.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADwAAAA9CAYAAADxoArXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAASQSURBVGhD7ZhJLyxRFMdPd5vnOYZGQsSOhURYCBYs2AkJiQi+gCWJhbe0YC8RwcrwBUgkQojYiEhYECxIDDHPM+/9j6q8arpK6bolnVK/5Ka6q+p23/89555z7nUUFRW90S/CKV1/DbZgq/NpDbtcLumTNfEQDLGrq6sUGhoq3bEenwRvbW1RUFCQdMd62EHL6tiCrY4t2OrYgq2OLdgsnp6e6Obmhi4vL/mKdn9/Lz39OUwtLSHy7u6Ozs/PKTs7m/Lz8ykhIYGOjo7I4XDwdW5ujqKioigkJISb2QgR/Pj4KH165+Xlha2XmJhI5eXlVFNTw/fx+xD69vZ/g7a/v08bGxssfH5+nvvgPRnRdb1hwRCbm5srfXu3alhYGGVmZlJdXR0LhJXVCA4OZsvu7OzQ3t4eTU1N8WQFBgby8/X1daGiha3h5eVlWlpaou3tbWpqaqKGhgYeuJZY8PDwQBcXF5SUlET/Jp+am5tpc3OTZmdnpTfEIkwwLBIbG0vDw8PsltfX19ITfcjC3W439fb28gTAW0QjTPDJyQm1tbWxC2PwvoIonpaWRj09PezqiAciESL46uqKOjo6qLCwkG5vb6W7nmCt6gWi09PTqaKigiO5SAwLhttlZWVRZWUlnZ6eSnc9gdjd3V0OPnqFI0/n5ORQcXGx0HxtWDCCUkFBgabrOZ1OWlhYoMbGRjo+PqaIiAjpiTpYFikpKVRbW8sWF4VhwXBhWPirdYughsnp6uqikZERiouLk56og5SH9/zGwhhISUkJR2U9gQruDNEDAwMczRGUtFwcywVWxn+IEm1IMNwYYr8DLJ2RkUHj4+PU2tr6pYujKkM5KipaG3bp19dX6dP3QM6G1eDiQ0NDqmfhSHOYVL8QjEGgQMCgfAHujMoMeReBzRuwMFKTsr42giHBSDNra2s+zT6qqoCAAC4wqqurOQ2p4euEesOwS6tZRgtsFEpLS6m/v5/y8vJU8zfAZK6srAjbQBgSDDfD9k653dMCg4dYVGUoQ7GG1SozJdjBybsnoxgSjEFgZ4MBIehAkNw+8vz8zNtGuHBVVRWXox9R9kcwxBrHHhkHBKIw7NKIoKOjoxQeHk6RkZHcoqOjPURjDSKfDg4Oqrow3kc/5W/Ac3AwIPIkxLBgWHZmZoYmJibYxWE5BCRlVIXblpWVceWk5sJ4H/3QHw3fJycnuST1K8EA1oOVcWIBt1WKldGzVtEPDb+BzUZ3dzfvmkQiRDDWMgr89vZ2jtp6d0TeQF9E5L6+Ps7xohEiGKA8RNRtaWnRvSP6CPocHh5SZ2cnpyKRrizjcrvdf6TPbJ2zszOvLqkHWAauu7i4yEc8OJaFxbSOamSLoggZGxvjUhNrHUHLDEw5l4ZACI6Pj6f6+nouMoBcMeEq5+6DgwNOPdPT01xCosY2E9MP4rG2U1NTKTk5mVMYhOIKcXB9pJ2YmBiO9qKKCy1MFSwD4XJBAXDFf6GZsU61EBa0tIDlIAzFCRoqJ1x/Wiz4EcH+hC3Y6tiCrY4t2OrYgq2OLdjq2IKtji3Y6vwywUR/AQlkL+ts25eCAAAAAElFTkSuQmCC)を押すと実行できます。\n",
        "\n",
        "\n",
        "<font color=\"red\">**初回起動時(タイムアウト後も含む?)は、上部「ランタイム」タブ⇒「ランタイムのタイプを変更」⇒「ハードウェアアクセラレータ」が\"GPU\"になっているか確認してください。**</font>\n",
        "\n",
        "\n",
        "▼最初はまずこの順で行ってください。(Ctrl+F9で「すべてのセルを実行」してもいいかもしれません)\n",
        "1.   パッケージのインストール\n",
        "2.   GoogleDriveのマウント\n",
        "3.   音源のパスを指定\n",
        "4.   whisperで文字起こし\\\n",
        "  4-1. モデルの読み込み\\\n",
        "  4-2. 文字起こし\\\n",
        "  (4-3. 文字起こしの結果の編集)\n",
        "5.   音源の分割\n",
        "\n",
        "▼連続して2回目以降音源を変えて実行する場合 (※ランタイムがタイムアウトした場合は1.から全部)\n",
        "3.  音源のパスを指定\n",
        "4.  4-2. を実行(4-1は不要)\n",
        "5.  音源の分割\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<補足>\n",
        "\n",
        "2.のマウントが成功すると、画面左側のファイルメニュー内に\"drive\"が表示されるので、展開して任意の音源ファイルを探してください。右クリックからパスのコピーができます。\n",
        "\n",
        "![folder_tab.png](data:image/png;base64,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)\n",
        "\n",
        "▲コレ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6FLDtMARTch"
      },
      "source": [
        "## **3. 音源のパスを指定**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TytLViU2W-wG",
        "outputId": "e99bb763-181e-4e29-b2b3-9c249f46c1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'meeting_demo_1half'を読み込みました。\n"
          ]
        }
      ],
      "source": [
        "#@title code { display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "#FILE_PATH = input(\"音源のファイルパスを入力してください(Enterで確定) >>\")\n",
        "FILE_PATH = \"../resource/meeting_voice/meeting_demo_1half.wav\"\n",
        "#FILE_PATH = \"../resource/onsei_kaigi1015/OGISmeeting.wav\"\n",
        "if (os.path.isfile(FILE_PATH)):\n",
        "  fmt = os.path.splitext(FILE_PATH)[1].replace(\".\",\"\")\n",
        "  FILE_NAME =  os.path.splitext(os.path.basename(FILE_PATH))[0]\n",
        "  DIR_NAME = os.path.dirname(FILE_PATH)\n",
        "  print( \"'\" + FILE_NAME + \"'\" + \"を読み込みました。\")\n",
        "else:\n",
        "  raise FileNotFoundError(\"指定のファイルパスが見つかりませんでした。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AsDsmOlXtRH"
      },
      "source": [
        "## **4. Whisperによる文字起こし処理**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwDEjbkRkVol"
      },
      "source": [
        "### **4-1. モデルの読み込み**\n",
        "\n",
        "* 一度実行した後は、同じランタイムである限り4-1はスキップして大丈夫です。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VcrwkHgXsrQ",
        "outputId": "df137990-c975-4c76-e93f-c123493f915c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "#@title code { display-mode: \"form\" }\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"large\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXIAygfckuGd"
      },
      "source": [
        "### **4-2. 文字起こし**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "XoFGM0nckkg4",
        "outputId": "5a359541-75a3-4e22-a341-4e2dca01b62c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2700/2700 [00:06<00:00, 392.05frames/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>5.76</td>\n",
              "      <td>これから新製品についての会議を始めようと思います</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.76</td>\n",
              "      <td>8.64</td>\n",
              "      <td>ターゲット層についてなんですが</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.64</td>\n",
              "      <td>14.46</td>\n",
              "      <td>新製品のターゲット層は30代から40代のビジネスマンに絞りましょう</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.46</td>\n",
              "      <td>17.84</td>\n",
              "      <td>高機能でスタイリッシュのデザインが有利になると思います</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.84</td>\n",
              "      <td>23.24</td>\n",
              "      <td>でも最近の若い人たちも同じ製品を求めている傾向があると</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>23.24</td>\n",
              "      <td>24.86</td>\n",
              "      <td>現場の声から聞いています</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>24.86</td>\n",
              "      <td>27.00</td>\n",
              "      <td>それに価格単位ももう少し</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   start    end                               text\n",
              "0   0.00   5.76           これから新製品についての会議を始めようと思います\n",
              "1   5.76   8.64                    ターゲット層についてなんですが\n",
              "2   8.64  14.46  新製品のターゲット層は30代から40代のビジネスマンに絞りましょう\n",
              "3  14.46  17.84        高機能でスタイリッシュのデザインが有利になると思います\n",
              "4  17.84  23.24        でも最近の若い人たちも同じ製品を求めている傾向があると\n",
              "5  23.24  24.86                       現場の声から聞いています\n",
              "6  24.86  27.00                       それに価格単位ももう少し"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title code { display-mode: \"form\" }\n",
        "import pandas as pd\n",
        "\n",
        "res = model.transcribe(FILE_PATH, verbose=False, language=\"ja\")\n",
        "\n",
        "\n",
        "speech_data = pd.DataFrame(res[\"segments\"])[[\"start\", \"end\", \"text\"]]\n",
        "pd.set_option(\"display.max_rows\", len(speech_data))\n",
        "speech_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbWbDzbKlIn"
      },
      "source": [
        "### **4-3. 文字起こし結果の編集(任意)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH-b66aWeZcP"
      },
      "source": [
        "実行すると文字起こしの結果を編集できます。\n",
        "\n",
        "<使用方法>\n",
        "\n",
        "\n",
        "1.   編集メニュー\\\n",
        "    ・行番号入力でその行の要素を編集できます。\\\n",
        "    ・コマンド'tr'で、全セリフから任意のワードを検索し、指定のワードで全置換する機能が使えます。\\\n",
        "    ・コマンド'd'で指定行を削除できます。\\\n",
        "    ・終了するには'e'を入力してください。\n",
        "\n",
        "2.   行編集\\\n",
        "    ・'s'で分割開始時間、'e'で分割終了時間、't'でセリフを編集できます。\\\n",
        "    ・行編集を終了するには、「入力なし」もしくは上記以外のコマンドを入力してください。\n",
        "\n",
        "3.   セリフ指定ワード置換(コマンド'tr')\\\n",
        "    ・「置換前ワード」を含むすべてのセリフについて、そのワードが出現する箇所をすべて「置換後ワード」に置き換えます。\\\n",
        "    ・自動文字起こしで、同じ言葉が同じ誤変換となった場合(固有名詞など)に便利です。\n",
        "\n",
        "  \n",
        "   <動作例>\n",
        "   \n",
        "         <BEFORE>                                                                        <AFTER>\n",
        "    0 \"鈴木が泳いでる\"                                                             0 \"スズキが泳いでる\"\n",
        "\n",
        "    1 \"回転寿司で鈴木を頼む\"     =====(置換前:\"鈴木\" / 置換後: \"スズキ\")====>      1 \"回転寿司でスズキを頼む\"\n",
        "\n",
        "    2 \"鈴木の軽自動車\"                                                             2 \"スズキの軽自動車\"\n",
        "\n",
        "    3 \"鈴木スズキススキ鈴木\"                                                       3 \"スズキスズキススキスズキ\"\n",
        "\n",
        "4.    行削除(コマンド'd')\\\n",
        "    ・指定した行を削除します。\\\n",
        "    ・行削除を終了するには'e'を入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "eBJ2Z3r6ij7F",
        "outputId": "4ef614a1-7717-4454-86a4-263c8efdc9c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            "    start    end                                              text\n",
            "0    0.70   5.24                新製品のターゲット層は30代から40代のビジネスマンに絞りましょう。\n",
            "1    6.08   8.22                         高機能でスタイリッシュなデザインが売りになります。\n",
            "2    9.56  13.98          でも最近の若い人たちも同じ製品を求めている傾向があると現場の声から聞いています。\n",
            "3   14.36  16.84                                  それに価格ももう少し広げた方が…\n",
            "4   16.84  19.68                            いや、30代以上に限定するべきだと思います。\n",
            "5   20.20  22.44                            若い層はどうせ似たようなものを買うでしょう?\n",
            "6   23.12  27.18                    私たちが開発した技術は幅広い年齢層にアピールできるものです。\n",
            "7   27.18  30.42                                  技術的にも他の層に響く可能性が…\n",
            "8   30.42  32.42                                 でもリソースは限られているんです。\n",
            "9   33.34  36.04                         今のマーケティング戦略を変更する余裕はありません。\n",
            "10  37.28  42.54  それは理解していますが、もし市場の動向が変わったら、最初から視野を狭めるのはリスクだと思います。\n",
            "11  43.40  46.96                              うーん、そこまで考える時間はないんです。\n",
            "12  47.64  48.96                                      今はこの路線で行きます。\n",
            "13  49.48  53.86                        わかりました。しかし、もし失敗したらその時の責任は…\n",
            "14  53.86  54.58                                           失敗しません!\n",
            "15  55.78  57.14                                      私たちはこれで行きます。\n",
            "16  57.18  59.18                                               はい。\n",
            "========================================================================================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "#@title code { display-mode: \"form\" }\n",
        "\n",
        "from pydub import *\n",
        "from pydub.playback import play\n",
        "import IPython.display as ipd\n",
        "import shutil\n",
        "import time\n",
        "import pyperclip as pc\n",
        "\n",
        "def isFloat(s):\n",
        "  try:\n",
        "    float(s)\n",
        "  except ValueError:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def printDataFrame():\n",
        "  print(\"=\"*200)\n",
        "  print(speech_data)\n",
        "  print(\"=\"*200)\n",
        "\n",
        "\n",
        "os.makedirs(\"tmp_wss\", exist_ok=True) #修正作業用の一時フォルダ\n",
        "\n",
        "source_file = AudioSegment.from_file(FILE_PATH, format=fmt)\n",
        "printDataFrame()\n",
        "idx = input(f\"◆編集メニュー: 修正するデータの行番号({speech_data.index[0]}～{speech_data.index[-1]})、またはコマンド('tr': セリフ指定ワード置換 / 'd': 行削除)を入力してください。'e'で終了します。>> \")\n",
        "\n",
        "\n",
        "while idx != 'e':\n",
        "  if idx.isnumeric():\n",
        "    idx=int(idx)\n",
        "    if (idx in speech_data.index):\n",
        "      splitted_sound = source_file[speech_data[\"start\"][idx]*1000 : speech_data[\"end\"][idx]*1000]\n",
        "      splitted_sound.export(\"tmp_wss/tmp.wav\")\n",
        "      print(\"▼指定の音声\")\n",
        "      ipd.display(ipd.Audio(\"tmp_wss/tmp.wav\", autoplay=True))\n",
        "      time.sleep(0.5)\n",
        "      while True:\n",
        "        colm = input(f\"修正箇所を指定(右記以外の入力で終了)⇒ 's':開始時間[sec] / 'e':終了時間[sec] / 't':セリフ>> \")\n",
        "        if(colm == 's'):\n",
        "          val = input(f\"開始時間を入力してください。(現在値: {speech_data.at[idx, 'start']}) [sec]>> \")\n",
        "          speech_data.at[idx, 'start'] = float(val) if isFloat(val) else speech_data.at[idx, 'start']\n",
        "        elif(colm == 'e'):\n",
        "          val = input(f\"終了時間を入力してください。(現在値: {speech_data.at[idx, 'start']}) [sec]>> \")\n",
        "          speech_data.at[idx, 'end'] = float(val) if isFloat(val) else speech_data.at[idx, 'end']\n",
        "        elif(colm == 't'):\n",
        "          val = input(f\"セリフを入力してください。(現在値: {speech_data.at[idx, 'text']})>> \")\n",
        "          speech_data.at[idx, 'text'] = val\n",
        "        else:\n",
        "          ipd.clear_output()\n",
        "          print(f\"▼修正完了: {idx}行目\")\n",
        "          print(f\"{speech_data['text'][idx]}\")\n",
        "          break\n",
        "    else:\n",
        "      ipd.clear_output()\n",
        "\n",
        "\n",
        "  elif(idx == 'tr'):\n",
        "    print(\"指定のワードが出現するすべてのセリフについて、出現箇所を任意のワードで置換します。\")\n",
        "    word_before = input(f\"置換前のワードを入力してください: >>\")\n",
        "    tmp_df = speech_data.query('text.str.contains(@word_before)', engine='python')\n",
        "    word_after = input(f\"置換後のワードを入力してください: >>\")\n",
        "    for i in tmp_df.index:\n",
        "      speech_data.at[i, 'text'] = speech_data.at[i, 'text'].replace(word_before, word_after)\n",
        "    ipd.clear_output()\n",
        "    print(f\"▼修正完了: {[(str(i)+'行目 ') for i in tmp_df.index]}\")\n",
        "    print(tmp_df)\n",
        "\n",
        "  elif(idx == 'd'):\n",
        "    while True:\n",
        "      speech_data = speech_data.reset_index(drop=True)\n",
        "      ipd.clear_output()\n",
        "      printDataFrame()\n",
        "      val = input(f\"削除する行番号({speech_data.index[0]}～{speech_data.index[-1]})を入力してください。'e'で終了します。>> \")\n",
        "      if val.isnumeric():\n",
        "        val = int(val)\n",
        "        if (val in speech_data.index):\n",
        "          speech_data = speech_data.drop(val) if input(f\"{val}行目を削除します。よろしければもう一度行番号({val})を入力してください。異なる入力でキャンセルできます。>>\") == str(val) else speech_data\n",
        "        else:\n",
        "          print(\"無効な値です。\")\n",
        "      elif (val == \"e\"):\n",
        "        ipd.clear_output()\n",
        "        break\n",
        "      else:\n",
        "        print(\"無効な値(コマンド)です。\")\n",
        "\n",
        "  else:\n",
        "    ipd.clear_output()\n",
        "    print(\"無効な値(コマンド)です。\")\n",
        "\n",
        "  printDataFrame()\n",
        "  idx = input(f\"◆編集メニュー: 修正するデータの行番号({speech_data.index[0]}～{speech_data.index[-1]})、またはコマンド('tr': セリフ指定ワード置換 / 'd': 行削除)を入力してください。'e'で終了します。>>\")\n",
        "\n",
        "\n",
        "shutil.rmtree(\"tmp_wss\")\n",
        "ipd.clear_output()\n",
        "speech_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KDbKWufZedv"
      },
      "source": [
        "## **6.素材の分割(pydub)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evyBZlsNtvl1"
      },
      "source": [
        "*  セリフで分割された音声(または動画)ファイルが生成されます(※)。音源と同一のディレクトリ内に\"(音源名)_split\"というフォルダが作成され、この中に出力されます。\n",
        "*  動画ファイルの分割を行う場合は出力ファイルのフレームレートを選択してください(手入力可)。フレームレートによりますが、<font color=\"red\">**出力に時間がかかります。**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ2PaEcoZdVM",
        "outputId": "ac7cf2bf-307e-4961-f1d2-32267adb5f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ◆◆ Process Completed. ◆◆\n"
          ]
        }
      ],
      "source": [
        "#@title code { display-mode: \"form\" }\n",
        "#@markdown ###動画または音声ファイルの分割処理を行わず，ＣＳＶファイルのみ出力する場合はチェックを入れてください．\n",
        "dnt_split = False #@param {type:\"boolean\"}\n",
        "import math\n",
        "from moviepy.editor import VideoFileClip\n",
        "sound_format_list = [\"mp3\", \"wav\", \"flac\", \"aac\", \"flac\", \"mp2\"]\n",
        "movie_format_list = [\"mp4\", \"mkv\", \"wmv\", \"m4v\", \"avi\", \"mpg\", \"mov\"]\n",
        "\n",
        "folder_num = 0\n",
        "\n",
        "export_dir = f\"{DIR_NAME}/{FILE_NAME}_split\"\n",
        "while os.path.isdir(export_dir):\n",
        "  folder_num += 1\n",
        "  export_dir = f\"{DIR_NAME}/{FILE_NAME}_split_{folder_num}\"\n",
        "os.mkdir(export_dir)\n",
        "\n",
        "if not dnt_split:\n",
        "  if fmt in sound_format_list:\n",
        "    source_file = AudioSegment.from_file(FILE_PATH, format=fmt)\n",
        "    for i in speech_data.index:\n",
        "      print(f\"\\r◆{math.ceil(((i+1)/len(speech_data))*100)}% - Exporting({i+1} of {len(speech_data)}): {i+1}_{speech_data['text'][i]}.{fmt}\",end='')\n",
        "      splitted_sound = source_file[speech_data[\"start\"][i]*1000 : speech_data[\"end\"][i]*1000]\n",
        "      splitted_sound.export(f\"{export_dir}/{i+1}-{speech_data['text'][i]}.{fmt}\")\n",
        "\n",
        "\n",
        "  elif fmt in movie_format_list:\n",
        "    sourceVideo = VideoFileClip(FILE_PATH)\n",
        "    #@markdown ###フレームレートを選択(手入力可)\n",
        "    FPS = \"30\" #@param [5, 23.98, 24, 25, 29.97, 30, 50, 59.94, 60, 119.88, 120, 240]{allow-input:true}\n",
        "    print(sourceVideo.fps,\"fps\")\n",
        "    for i in speech_data.index:\n",
        "      print(f\"\\r◆{math.ceil(((i+1)/len(speech_data))*100)}% - Exporting({i+1} of {len(speech_data)}): {i+1}_{speech_data['text'][i]}.{fmt}\"+\"\\033[9A\",end='')\n",
        "      vid = sourceVideo.subclip(speech_data[\"start\"][i], speech_data[\"end\"][i])\n",
        "      vid.write_videofile(f\"{export_dir}/{i+1}-{speech_data['text'][i]}.{fmt}\", fps=float(FPS))\n",
        "      ipd.clear_output()\n",
        "\n",
        "\n",
        "  else:\n",
        "    print(\"拡張子を確認してください。\")\n",
        "\n",
        "speech_data.to_csv(f\"{export_dir}/data.csv\", index=True)\n",
        "\n",
        "print(\"\\r ◆◆ Process Completed. ◆◆\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **new1.話者識別テスト**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import contextlib\n",
        "import wave\n",
        "#from pyannote.audio import Pipeline\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment, notebook\n",
        "\n",
        "def segment_embedding(\n",
        "    file_name: str,\n",
        "    duration: float,\n",
        "    segment,\n",
        "    embedding_model: PretrainedSpeakerEmbedding\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    音声ファイルから指定されたセグメントの埋め込みを計算します。\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name: str\n",
        "        音声ファイルのパス\n",
        "    duration: float\n",
        "        音声ファイルの継続時間\n",
        "    segment: whisperのtranscribeのsegment\n",
        "    embedding_model: PretrainedSpeakerEmbedding\n",
        "        埋め込みモデル\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        計算された埋め込みベクトル\n",
        "    \"\"\"\n",
        "    audio = Audio()\n",
        "    start = segment[\"start\"]\n",
        "    end = min(duration, segment[\"end\"])\n",
        "    clip = Segment(start, end)\n",
        "    waveform, sample_rate = audio.crop(file_name, clip)\n",
        "    #print(waveform)\n",
        "    return embedding_model(waveform[None])\n",
        "\n",
        "def generate_speaker_embeddings(\n",
        "    meeting_file_path: str,\n",
        "    transcript\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    音声ファイルから話者の埋め込みを計算します。\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    meeting_file_path: str\n",
        "        音声ファイルのパス\n",
        "    transcript: Whisper API の transcribe メソッドの出力結果\n",
        "\n",
        "    Returns\n",
        "    \n",
        "    -------\n",
        "    np.ndarray\n",
        "        計算された話者の埋め込み群\n",
        "    \"\"\"\n",
        "    segments = transcript['segments']\n",
        "    embedding_model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\", device='cpu')\n",
        "    embeddings = np.zeros(shape=(len(segments), 192))\n",
        "\n",
        "    with contextlib.closing(wave.open(meeting_file_path, 'r')) as f:\n",
        "        frames = f.getnframes()\n",
        "        rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "\n",
        "    for i, segment in enumerate(segments):\n",
        "        embeddings[i] = segment_embedding(meeting_file_path, duration, segment, embedding_model)\n",
        "\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from typing import List, Tuple\n",
        "\n",
        "def clustering_embeddings(speaker_count: int, embeddings: np.ndarray) -> AgglomerativeClustering:\n",
        "    \"\"\"\n",
        "    埋め込みデータをクラスタリングして、クラスタリングオブジェクトを返します。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embeddings: np.ndarray\n",
        "        分散表現（埋め込み）のリスト。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    AgglomerativeClustering\n",
        "        クラスタリングオブジェクト。\n",
        "    \"\"\"\n",
        "    clustering = AgglomerativeClustering(speaker_count).fit(embeddings)\n",
        "    return clustering\n",
        "\n",
        "def format_speaker_output_by_segment(clustering: AgglomerativeClustering, transcript: dict) -> str:\n",
        "    \"\"\"\n",
        "    クラスタリングの結果をもとに、各発話者ごとにセグメントを整形して出力します\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    clustering: AgglomerativeClustering\n",
        "        クラスタリングオブジェクト。\n",
        "    transcript: dict\n",
        "        Whisper API の transcribe メソッドの出力結果\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        発話者ごとに整形されたセグメントの文字列\n",
        "    \"\"\"\n",
        "    labeled_segments = []\n",
        "    for label, segment in zip(clustering.labels_, transcript[\"segments\"]):\n",
        "        labeled_segments.append((label, segment[\"start\"], segment[\"text\"]))\n",
        "\n",
        "    output = \"\"\n",
        "    for speaker, _, text in labeled_segments:\n",
        "        output += f\"話者{speaker + 1}: 「{text}」\\n\"\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def closest_reference_speaker(embedding: np.ndarray, references: List[Tuple[str, np.ndarray]]) -> str:\n",
        "    \"\"\"\n",
        "    与えられた埋め込みに最も近い参照話者を返します。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding: np.ndarray\n",
        "        話者の埋め込み\n",
        "    references: List[Tuple[str, np.ndarray]]\n",
        "        参照話者の名前と埋め込みのリスト\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        最も近い参照話者の名前\n",
        "    \"\"\"\n",
        "    min_distance = float('inf')\n",
        "    closest_speaker = None\n",
        "    for name, reference_embedding in references:\n",
        "        #print(reference_embedding.shape)\n",
        "        reference_embedding = reference_embedding[0]\n",
        "        distance = cosine(embedding, reference_embedding)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_speaker = name\n",
        "\n",
        "    return closest_speaker\n",
        "\n",
        "def format_speaker_output_by_segment2(embeddings: np.ndarray, transcript: dict, reference_embeddings: List[Tuple[str, np.ndarray]]) -> str:\n",
        "    \"\"\"\n",
        "    各発話者の埋め込みに基づいて、セグメントを整形して出力します。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embeddings: np.ndarray\n",
        "        話者の埋め込みのリスト\n",
        "    transcript: dict\n",
        "        Whisper API の transcribe メソッドの出力結果\n",
        "    reference_embeddings: List[Tuple[str, np.ndarray]]\n",
        "        参照話者の名前と埋め込みのリスト\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        発話者ごとに整形されたセグメントの文字列。\n",
        "    \"\"\"\n",
        "    labeled_segments = []\n",
        "    for embedding, segment in zip(embeddings, transcript[\"segments\"]):\n",
        "        #print(embedding.shape)\n",
        "        speaker_name = closest_reference_speaker(embedding, reference_embeddings)\n",
        "        labeled_segments.append((speaker_name, segment[\"start\"], segment[\"text\"]))\n",
        "\n",
        "    output = \"\"\n",
        "    for speaker, _, text in labeled_segments:\n",
        "        output += f\"{speaker}: 「{text}」\\n\"\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### new1-2 **実行部分テスト**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7037/7037 [00:18<00:00, 385.85frames/s]\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  stats = torch.load(path, map_location=device)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'話者1: 「今日これから新製品についての会議を始めます。」\\n話者1: 「ターゲット層についてなんですが、」\\n話者1: 「新製品のターゲット層は30代から40代のビジネスマンに絞りましょう。」\\n話者1: 「高企業でスタイリッシュのデザインが有利になると思います。」\\n話者1: 「でも最近の若い人たちも同じ製品を求めている傾向があると現場の声が聞こえます。」\\n話者1: 「それに価格単位をもう少し広げた方が、」\\n話者1: 「いや、30代以上に限定するべきだと思います。」\\n話者2: 「若い層がどうせ似たようなものを買うでしょう。」\\n話者1: 「私たちが開発した技術は幅広いデザイン層でうっきりできるものです。」\\n話者1: 「技術的にも他の層に響く可能性があります。」\\n話者1: 「でも、2層しか限られているんです。」\\n話者2: 「今のパワーリーティング戦でこれでもこうする余裕はありません。」\\n話者1: 「それは理解していますが、」\\n話者1: 「もし市場の動向が変わったら、」\\n話者1: 「最初から知恵を狭めるようなリスクがあると思います。」\\n話者1: 「うーん、」\\n話者1: 「考える時間はないんです。」\\n話者2: 「今はこの路線で行きます。」\\n話者1: 「わかりました。」\\n話者1: 「しかし、もし失敗したらその時の責任は?」\\n話者1: 「失敗しません。」\\n話者1: 「私たちはこれで行きます。」\\n話者1: 「今日の会議は終わりです。」\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#会議音声のクラスタリング\n",
        "FILE_PATH = \"../resource/meeting_voice/meeting_demo_special_record_deluxe.wav\"\n",
        "#FILE_PATH = \"../resource/onsei_kaigi1015/OGISmeeting.wav\"\n",
        "#FILE_PATH = \"../resource/output.wav\"\n",
        "num_people = 2  #話者の数\n",
        "\n",
        "import pandas as pd\n",
        "res_meet = model.transcribe(FILE_PATH, verbose=False, language=\"ja\")\n",
        "\n",
        "embed_meet = generate_speaker_embeddings(FILE_PATH,res_meet)\n",
        "\n",
        "clus_embed_meet = clustering_embeddings(num_people,embed_meet)\n",
        "format_out_meet = format_speaker_output_by_segment(clus_embed_meet, res_meet)\n",
        "format_out_meet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [00:01<00:00, 652.93frames/s]\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  stats = torch.load(path, map_location=device)\n",
            "100%|██████████| 1339/1339 [00:02<00:00, 463.96frames/s]\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  stats = torch.load(path, map_location=device)\n",
            "100%|██████████| 900/900 [00:01<00:00, 538.61frames/s]\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n",
            "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  stats = torch.load(path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "#各話者の参考音声のembedding\n",
        "FILE_PATH_DICT = {\"hanagata\": \"../resource/onsei_kaigi1015/hanagata_sample_voice.wav\", \"isizawa\": \"../resource/meeting_voice/ishizawa_sample.wav\",\n",
        "                  \"shibasaki\": \"../resource/onsei_kaigi1015/shibasaki_sample.wav\"}\n",
        "                  #\"takeda\": \"../resource/takeda_sample_voice.wav\"}\n",
        "#num_people = 1  #話者の数\n",
        "ref = []\n",
        "import pandas as pd\n",
        "for name, file in FILE_PATH_DICT.items():\n",
        "    res = model.transcribe(file, verbose=False, language=\"ja\")\n",
        "    embed = generate_speaker_embeddings(file,res)\n",
        "    #clus_embed = clustering_embeddings(num_people, embed)\n",
        "    #format_out = format_speaker_output_by_segment(clus_embed, res)\n",
        "    ref.append((name,embed))\n",
        "close_ref_speaker = format_speaker_output_by_segment2(embed_meet,res_meet,ref)\n",
        "\n",
        "#print(close_ref_speaker)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hanagata: 「今日これから新製品についての会議を始めます。」\n",
            "hanagata: 「ターゲット層についてなんですが、」\n",
            "isizawa: 「新製品のターゲット層は30代から40代のビジネスマンに絞りましょう。」\n",
            "isizawa: 「高企業でスタイリッシュのデザインが有利になると思います。」\n",
            "isizawa: 「でも最近の若い人たちも同じ製品を求めている傾向があると現場の声が聞こえます。」\n",
            "isizawa: 「それに価格単位をもう少し広げた方が、」\n",
            "isizawa: 「いや、30代以上に限定するべきだと思います。」\n",
            "isizawa: 「若い層がどうせ似たようなものを買うでしょう。」\n",
            "shibasaki: 「私たちが開発した技術は幅広いデザイン層でうっきりできるものです。」\n",
            "shibasaki: 「技術的にも他の層に響く可能性があります。」\n",
            "isizawa: 「でも、2層しか限られているんです。」\n",
            "isizawa: 「今のパワーリーティング戦でこれでもこうする余裕はありません。」\n",
            "isizawa: 「それは理解していますが、」\n",
            "hanagata: 「もし市場の動向が変わったら、」\n",
            "isizawa: 「最初から知恵を狭めるようなリスクがあると思います。」\n",
            "hanagata: 「うーん、」\n",
            "hanagata: 「考える時間はないんです。」\n",
            "isizawa: 「今はこの路線で行きます。」\n",
            "shibasaki: 「わかりました。」\n",
            "shibasaki: 「しかし、もし失敗したらその時の責任は?」\n",
            "isizawa: 「失敗しません。」\n",
            "isizawa: 「私たちはこれで行きます。」\n",
            "isizawa: 「今日の会議は終わりです。」\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(close_ref_speaker)\n",
        "with open(\"../resource/out_puts/test.txt\", 'w', encoding=\"utf-8\") as file:\n",
        "    file.write(close_ref_speaker)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv3_12_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
