{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル読み込みと関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import whisper \n",
    "model = whisper.load_model(\"large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各セグメントのembeddingを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import wave\n",
    "#from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment, notebook\n",
    "\n",
    "def segment_embedding(\n",
    "    file_name: str,\n",
    "    duration: float,\n",
    "    segment,\n",
    "    embedding_model: PretrainedSpeakerEmbedding\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    音声ファイルから指定されたセグメントの埋め込みを計算します。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name: str\n",
    "        音声ファイルのパス\n",
    "    duration: float\n",
    "        音声ファイルの継続時間\n",
    "    segment: whisperのtranscribeのsegment\n",
    "    embedding_model: PretrainedSpeakerEmbedding\n",
    "        埋め込みモデル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        計算された埋め込みベクトル\n",
    "    \"\"\"\n",
    "    audio = Audio()\n",
    "    start = segment[\"start\"]\n",
    "    end = min(duration, segment[\"end\"])\n",
    "    clip = Segment(start, end)\n",
    "    waveform, sample_rate = audio.crop(file_name, clip)\n",
    "    #print(waveform)\n",
    "    return embedding_model(waveform[None])\n",
    "\n",
    "def generate_speaker_embeddings(\n",
    "    meeting_file_path: str,\n",
    "    transcript\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    音声ファイルから話者の埋め込みを計算します。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    meeting_file_path: str\n",
    "        音声ファイルのパス\n",
    "    transcript: Whisper API の transcribe メソッドの出力結果\n",
    "\n",
    "    Returns\n",
    "    \n",
    "    -------\n",
    "    np.ndarray\n",
    "        計算された話者の埋め込み群\n",
    "    \"\"\"\n",
    "    segments = transcript['segments']\n",
    "    embedding_model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\", device='cuda')\n",
    "    embeddings = np.zeros(shape=(len(segments), 192))\n",
    "\n",
    "    with contextlib.closing(wave.open(meeting_file_path, 'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        embeddings[i] = segment_embedding(meeting_file_path, duration, segment, embedding_model)\n",
    "\n",
    "    embeddings = np.nan_to_num(embeddings)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 埋め込みから各セグをクラスタリング(照合するなら無視でok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from typing import List, Tuple\n",
    "\n",
    "def clustering_embeddings(speaker_count: int, embeddings: np.ndarray) -> AgglomerativeClustering:\n",
    "    \"\"\"\n",
    "    埋め込みデータをクラスタリングして、クラスタリングオブジェクトを返します。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings: np.ndarray\n",
    "        分散表現（埋め込み）のリスト。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AgglomerativeClustering\n",
    "        クラスタリングオブジェクト。\n",
    "    \"\"\"\n",
    "    clustering = AgglomerativeClustering(speaker_count).fit(embeddings)\n",
    "    return clustering\n",
    "\n",
    "def format_speaker_output_by_segment(clustering: AgglomerativeClustering, transcript: dict) -> str:\n",
    "    \"\"\"\n",
    "    クラスタリングの結果をもとに、各発話者ごとにセグメントを整形して出力します\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clustering: AgglomerativeClustering\n",
    "        クラスタリングオブジェクト。\n",
    "    transcript: dict\n",
    "        Whisper API の transcribe メソッドの出力結果\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        発話者ごとに整形されたセグメントの文字列\n",
    "    \"\"\"\n",
    "    labeled_segments = []\n",
    "    for label, segment in zip(clustering.labels_, transcript[\"segments\"]):\n",
    "        labeled_segments.append((label, segment[\"start\"], segment[\"text\"]))\n",
    "\n",
    "    output = \"\"\n",
    "    for speaker, _, text in labeled_segments:\n",
    "        output += f\"話者{speaker + 1}: 「{text}」\\n\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各embeddingごとにサンプル音声とcos類似度で照合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from typing import List, Tuple\n",
    "\n",
    "def closest_reference_speaker(embedding: np.ndarray, references: List[Tuple[str, np.ndarray]]) -> str:\n",
    "    \"\"\"\n",
    "    与えられた埋め込みに最も近い参照話者を返します。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding: np.ndarray\n",
    "        話者の埋め込み\n",
    "    references: List[Tuple[str, np.ndarray]]\n",
    "        参照話者の名前と埋め込みのリスト\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        最も近い参照話者の名前\n",
    "    \"\"\"\n",
    "    min_distance = float('inf')\n",
    "    closest_speaker = None\n",
    "    for name, reference_embedding in references:\n",
    "        #print(reference_embedding.shape)\n",
    "        reference_embedding = reference_embedding[0]\n",
    "        distance = cosine(embedding, reference_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_speaker = name\n",
    "\n",
    "    return closest_speaker\n",
    "\n",
    "def format_speaker_output_by_segment2(embeddings: np.ndarray, transcript: dict, reference_embeddings: List[Tuple[str, np.ndarray]]) -> str:\n",
    "    \"\"\"\n",
    "    各発話者の埋め込みに基づいて、セグメントを整形して出力します。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings: np.ndarray\n",
    "        話者の埋め込みのリスト\n",
    "    transcript: dict\n",
    "        Whisper API の transcribe メソッドの出力結果\n",
    "    reference_embeddings: List[Tuple[str, np.ndarray]]\n",
    "        参照話者の名前と埋め込みのリスト\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        発話者ごとに整形されたセグメントの文字列。\n",
    "    \"\"\"\n",
    "    labeled_segments = []\n",
    "    for embedding, segment in zip(embeddings, transcript[\"segments\"]):\n",
    "        #print(embedding.shape)\n",
    "        speaker_name = closest_reference_speaker(embedding, reference_embeddings)\n",
    "        labeled_segments.append((speaker_name, segment[\"start\"], segment[\"text\"]))\n",
    "\n",
    "    output = \"\"\n",
    "    for speaker, _, text in labeled_segments:\n",
    "        output += f\"{speaker}: 「{text}」\\n\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字起こしと話者識別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クラスタリングver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6911/6911 [00:17<00:00, 391.89frames/s]\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "話者1: 「ではこれから新製品のマーケティング戦略に関する」\n",
      "話者1: 「社内会議を始めます」\n",
      "話者1: 「まず新製品のターゲット層についてなんですが」\n",
      "話者1: 「30代から40代のビジネスマンに絞りましょう」\n",
      "話者1: 「高機能でサイリッシュなデザインが売りになります」\n",
      "話者1: 「でも最近の若い人たちも」\n",
      "話者1: 「同じ製品を求めている傾向があると」\n",
      "話者1: 「現場の声から聞いています」\n",
      "話者1: 「それに価格帯ももう少し広げた方が」\n",
      "話者1: 「いや30代以上に限定すべきだと思います」\n",
      "話者1: 「若い層はどうせ似たようなものを買うでしょう」\n",
      "話者2: 「私たちが開発した技術は」\n",
      "話者2: 「幅広い年齢層にアピールできるものです」\n",
      "話者2: 「技術的にも他の層に響く可能性が」\n",
      "話者1: 「でもリソースが限られているんです」\n",
      "話者1: 「今のマーケティング戦略を変更する余裕はありません」\n",
      "話者1: 「それは理解していますが」\n",
      "話者1: 「もし市場の動向が変わったら」\n",
      "話者1: 「最初からシェアを狭めるのはリスクだと思います」\n",
      "話者1: 「うーんそこまで考える時間はないんです」\n",
      "話者1: 「今はこの路線で行きます」\n",
      "話者3: 「はい」\n",
      "話者2: 「わかりました」\n",
      "話者2: 「しかしもし失敗したらその時の責任は」\n",
      "話者1: 「失敗しません」\n",
      "話者1: 「私たちはこれで行きます」\n",
      "話者1: 「はい以上です」\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#文字起こしする音声\n",
    "FILE_PATH = \"../resource/record241023/meeting_take2_mono.wav\"\n",
    "#FILE_PATH = \"../resource/meeting_voice/meeting_demo_special_record_deluxe.wav\"\n",
    "#FILE_PATH = \"../resource/onsei_kaigi1015/OGISmeeting.wav\"\n",
    "#FILE_PATH = \"../resource/output.wav\"\n",
    "num_people = 3  #話者の数\n",
    "\n",
    "\n",
    "res_meet = model.transcribe(FILE_PATH, verbose=False, language=\"ja\")\n",
    "#print(res_meet)\n",
    "embed_meet = generate_speaker_embeddings(FILE_PATH,res_meet)\n",
    "\n",
    "clus_embed_meet = clustering_embeddings(num_people,embed_meet)\n",
    "format_out_meet = format_speaker_output_by_segment(clus_embed_meet, res_meet)\n",
    "print(format_out_meet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 照合ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6911/6911 [00:17<00:00, 405.49frames/s]\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "100%|██████████| 765/765 [00:02<00:00, 336.32frames/s]\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "100%|██████████| 1031/1031 [00:02<00:00, 402.82frames/s]\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "100%|██████████| 1264/1264 [00:03<00:00, 388.75frames/s]\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/home/kbylab/ogis2024/venv3_12_1/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanagata: 「ではこれから新製品のマーケティング戦略に関する」\n",
      "hanagata: 「社内会議を始めます」\n",
      "hanagata: 「まず新製品のターゲット層についてなんですが」\n",
      "hanagata: 「30代から40代のビジネスマンに絞りましょう」\n",
      "hanagata: 「高機能でサイリッシュなデザインが売りになります」\n",
      "isizawa: 「でも最近の若い人たちも」\n",
      "isizawa: 「同じ製品を求めている傾向があると」\n",
      "isizawa: 「現場の声から聞いています」\n",
      "isizawa: 「それに価格帯ももう少し広げた方が」\n",
      "hanagata: 「いや30代以上に限定すべきだと思います」\n",
      "hanagata: 「若い層はどうせ似たようなものを買うでしょう」\n",
      "shibasaki: 「私たちが開発した技術は」\n",
      "shibasaki: 「幅広い年齢層にアピールできるものです」\n",
      "shibasaki: 「技術的にも他の層に響く可能性が」\n",
      "hanagata: 「でもリソースが限られているんです」\n",
      "hanagata: 「今のマーケティング戦略を変更する余裕はありません」\n",
      "isizawa: 「それは理解していますが」\n",
      "hanagata: 「もし市場の動向が変わったら」\n",
      "isizawa: 「最初からシェアを狭めるのはリスクだと思います」\n",
      "hanagata: 「うーんそこまで考える時間はないんです」\n",
      "hanagata: 「今はこの路線で行きます」\n",
      "hanagata: 「はい」\n",
      "hanagata: 「わかりました」\n",
      "shibasaki: 「しかしもし失敗したらその時の責任は」\n",
      "hanagata: 「失敗しません」\n",
      "hanagata: 「私たちはこれで行きます」\n",
      "hanagata: 「はい以上です」\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#文字起こしする音声\n",
    "FILE_PATH = \"../resource/record241023/meeting_take2_mono.wav\"\n",
    "# FILE_PATH = \"../resource/meeting_voice/meeting_demo_special_record_deluxe.wav\"\n",
    "\n",
    "#照合用個人の音声\n",
    "FILE_PATH_DICT = {\"hanagata\": \"../resource/record241023/sample_hanagata.wav\", \n",
    "                  \"isizawa\": \"../resource/record241023/sample_ishizawa.wav\",\n",
    "                  \"shibasaki\": \"../resource/record241023/sample_shibasaki.wav\"}\n",
    "# FILE_PATH_DICT = {\"hanagata\": \"../resource/onsei_kaigi1015/hanagata_sample_voice.wav\", \n",
    "#                   \"isizawa\": \"../resource/meeting_voice/ishizawa_sample.wav\",\n",
    "#                   \"shibasaki\": \"../resource/onsei_kaigi1015/shibasaki_sample.wav\"}\n",
    "\n",
    "\n",
    "ref = []\n",
    "res_meet = model.transcribe(FILE_PATH, verbose=False, language=\"ja\")\n",
    "embed_meet = generate_speaker_embeddings(FILE_PATH,res_meet)\n",
    "\n",
    "for name, file in FILE_PATH_DICT.items():\n",
    "    res = model.transcribe(file, verbose=False, language=\"ja\")\n",
    "    embed = generate_speaker_embeddings(file,res)\n",
    "    #clus_embed = clustering_embeddings(num_people, embed)\n",
    "    #format_out = format_speaker_output_by_segment(clus_embed, res)\n",
    "    ref.append((name,embed))\n",
    "close_ref_speaker = format_speaker_output_by_segment2(embed_meet,res_meet,ref)\n",
    "\n",
    "print(close_ref_speaker)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanagata: 「今日これから新製品についての会議を始めます。」\n",
      "hanagata: 「ターゲット層についてなんですが、」\n",
      "isizawa: 「新製品のターゲット層は30代から40代のビジネスマンに絞りましょう。」\n",
      "isizawa: 「高企業でスタイリッシュのデザインが有利になると思います。」\n",
      "isizawa: 「でも最近の若い人たちも同じ製品を求めている傾向があると現場の声が聞こえます。」\n",
      "isizawa: 「それに価格単位をもう少し広げた方が、」\n",
      "isizawa: 「いや、30代以上に限定するべきだと思います。」\n",
      "isizawa: 「若い層がどうせ似たようなものを買うでしょう。」\n",
      "shibasaki: 「私たちが開発した技術は幅広いデザイン層でうっきりできるものです。」\n",
      "shibasaki: 「技術的にも他の層に響く可能性があります。」\n",
      "isizawa: 「でも、2層しか限られているんです。」\n",
      "isizawa: 「今のパワーリーティング戦でこれでもこうする余裕はありません。」\n",
      "isizawa: 「それは理解していますが、」\n",
      "hanagata: 「もし市場の動向が変わったら、」\n",
      "isizawa: 「最初から知恵を狭めるようなリスクがあると思います。」\n",
      "hanagata: 「うーん、」\n",
      "hanagata: 「考える時間はないんです。」\n",
      "isizawa: 「今はこの路線で行きます。」\n",
      "shibasaki: 「わかりました。」\n",
      "shibasaki: 「しかし、もし失敗したらその時の責任は?」\n",
      "isizawa: 「失敗しません。」\n",
      "isizawa: 「私たちはこれで行きます。」\n",
      "isizawa: 「今日の会議は終わりです。」\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(close_ref_speaker)\n",
    "with open(\"../resource/out_puts/test.txt\", 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(close_ref_speaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####csv ver\n",
    "speech_data = pd.DataFrame(res[\"segments\"])[[\"start\", \"end\", \"text\"]]\n",
    "pd.set_option(\"display.max_rows\", len(speech_data))\n",
    "speech_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3_12_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
